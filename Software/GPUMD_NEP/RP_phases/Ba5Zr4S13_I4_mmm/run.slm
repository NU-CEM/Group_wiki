#!/bin/bash

# Slurm job options (job-name, compute nodes, job time)
#SBATCH --job-name=Example_MPI_Job
#SBATCH --time=02:30:00
#SBATCH --nodes=32
#SBATCH --tasks-per-node=128
#SBATCH --cpus-per-task=1

# Replace [budget code] below with your budget code (e.g. t01)
#SBATCH --account=e05-power-luc
#SBATCH --partition=standard
#SBATCH --qos=standard
#SBATCH --export=none

# Load compilation time environment
module load cpe/21.09
module swap PrgEnv-cray PrgEnv-gnu
export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH

# Set the number of threads to 1
#   This prevents any threaded system libraries from automatically
#   using threading.
export OMP_NUM_THREADS=1

# Launch the parallel job
#   Using 512 MPI processes and 128 MPI processes per node
#   srun picks up the distribution from the sbatch options

ulimit -s unlimited

srun -distribution=block:block --hint=nomultithread /work/e05/e05/prakaya/chalcogenide_perovskites/aims.220619.scalapack.mpi.x > aims.out
